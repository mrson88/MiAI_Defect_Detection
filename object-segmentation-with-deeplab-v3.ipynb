{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Import libraries"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:26.072957Z","iopub.status.busy":"2024-10-09T13:59:26.072059Z","iopub.status.idle":"2024-10-09T13:59:26.079068Z","shell.execute_reply":"2024-10-09T13:59:26.077945Z","shell.execute_reply.started":"2024-10-09T13:59:26.072909Z"},"id":"j89Cob0leL94","trusted":true},"outputs":[],"source":["import os\n","import cv2\n","from glob import glob\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{},"source":["## Hyper parameters"]},{"cell_type":"code","execution_count":101,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:26.081691Z","iopub.status.busy":"2024-10-09T13:59:26.081357Z","iopub.status.idle":"2024-10-09T13:59:26.089946Z","shell.execute_reply":"2024-10-09T13:59:26.088909Z","shell.execute_reply.started":"2024-10-09T13:59:26.081658Z"},"id":"k3pxqzekhKsm","trusted":true},"outputs":[],"source":["IMAGE_SIZE = 512\n","BATCH_SIZE = 2\n","NUM_CLASSES = 20\n","DATA_DIR = \"/home/mrson/python_code/hoc_deeplearning/MiAI_Defect_Detection/dataset/instance-level_human_parsing/Training\"\n","NUM_TRAIN_IMAGES = 1000\n","NUM_VAL_IMAGES = 50"]},{"cell_type":"markdown","metadata":{},"source":["## load data"]},{"cell_type":"code","execution_count":102,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:26.091923Z","iopub.status.busy":"2024-10-09T13:59:26.091323Z","iopub.status.idle":"2024-10-09T13:59:26.555638Z","shell.execute_reply":"2024-10-09T13:59:26.554685Z","shell.execute_reply.started":"2024-10-09T13:59:26.091887Z"},"id":"_elvLULpieQi","trusted":true},"outputs":[],"source":["train_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[:NUM_TRAIN_IMAGES]\n","train_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[:NUM_TRAIN_IMAGES]\n","\n","val_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[NUM_TRAIN_IMAGES:NUM_TRAIN_IMAGES+NUM_VAL_IMAGES]\n","val_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[NUM_TRAIN_IMAGES:NUM_TRAIN_IMAGES+NUM_VAL_IMAGES]"]},{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:26.557441Z","iopub.status.busy":"2024-10-09T13:59:26.557029Z","iopub.status.idle":"2024-10-09T13:59:26.566067Z","shell.execute_reply":"2024-10-09T13:59:26.565053Z","shell.execute_reply.started":"2024-10-09T13:59:26.557403Z"},"id":"hBpZAzdrliBV","trusted":true},"outputs":[],"source":["def read_image(image_path, mask=False):\n","    image = tf.io.read_file(image_path)\n","    if mask:\n","        image = tf.image.decode_png(image, channels=1)\n","        image.set_shape([None, None, 1])\n","        image  = tf.image.resize(images =  image, size=[IMAGE_SIZE, IMAGE_SIZE])\n","    else:\n","        image = tf.image.decode_png(image, channels=3)\n","        image.set_shape([None, None, 3])\n","        image  = tf.image.resize(images =  image, size=[IMAGE_SIZE, IMAGE_SIZE])\n","        image = tf.keras.applications.resnet50.preprocess_input(image)\n","    return image\n"]},{"cell_type":"code","execution_count":104,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:26.569438Z","iopub.status.busy":"2024-10-09T13:59:26.569070Z","iopub.status.idle":"2024-10-09T13:59:26.578606Z","shell.execute_reply":"2024-10-09T13:59:26.577735Z","shell.execute_reply.started":"2024-10-09T13:59:26.569388Z"},"id":"TsvBtAXiluMO","trusted":true},"outputs":[],"source":["def load_data(image_list, mask_list):\n","    image = read_image(image_list)\n","    mask = read_image(mask_list, mask=True)\n","    return image, mask"]},{"cell_type":"code","execution_count":105,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:26.582148Z","iopub.status.busy":"2024-10-09T13:59:26.579674Z","iopub.status.idle":"2024-10-09T13:59:26.590081Z","shell.execute_reply":"2024-10-09T13:59:26.589204Z","shell.execute_reply.started":"2024-10-09T13:59:26.582112Z"},"id":"MGvAisrgn-Y-","trusted":true},"outputs":[],"source":["def data_generator(image_list, mask_list):\n","    \n","    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n","    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","    return dataset"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:26.591793Z","iopub.status.busy":"2024-10-09T13:59:26.591431Z","iopub.status.idle":"2024-10-09T13:59:26.757472Z","shell.execute_reply":"2024-10-09T13:59:26.756586Z","shell.execute_reply.started":"2024-10-09T13:59:26.591758Z"},"id":"oe6TuIMzpFmg","trusted":true},"outputs":[],"source":["train_dataset = data_generator(train_images, train_masks)\n","val_dataset = data_generator(val_images, val_masks)\n"]},{"cell_type":"code","execution_count":107,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:26.759443Z","iopub.status.busy":"2024-10-09T13:59:26.759071Z","iopub.status.idle":"2024-10-09T13:59:26.765792Z","shell.execute_reply":"2024-10-09T13:59:26.764782Z","shell.execute_reply.started":"2024-10-09T13:59:26.759406Z"},"id":"GAwhIHrtpS9M","outputId":"4b9958db-641a-45ba-8559-8d7c2536f6b5","trusted":true},"outputs":[{"data":{"text/plain":["<_BatchDataset element_spec=(TensorSpec(shape=(2, 512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(2, 512, 512, 1), dtype=tf.float32, name=None))>"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset"]},{"cell_type":"markdown","metadata":{},"source":["## creat model"]},{"cell_type":"code","execution_count":108,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:26.768012Z","iopub.status.busy":"2024-10-09T13:59:26.767215Z","iopub.status.idle":"2024-10-09T13:59:26.774501Z","shell.execute_reply":"2024-10-09T13:59:26.773496Z","shell.execute_reply.started":"2024-10-09T13:59:26.767970Z"},"id":"VrGG1J94p-Lh","trusted":true},"outputs":[],"source":["def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding='same', use_bias=False,):\n","    x = layers.Conv2D(num_filters, kernel_size=kernel_size, dilation_rate = dilation_rate, padding= \"same\", use_bias= use_bias,kernel_initializer=keras.initializers.HeNormal(),\n",")(block_input)\n","    x = layers.BatchNormalization()(x)\n","    return tf.nn.relu(x)"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:26.776062Z","iopub.status.busy":"2024-10-09T13:59:26.775747Z","iopub.status.idle":"2024-10-09T13:59:26.785213Z","shell.execute_reply":"2024-10-09T13:59:26.784320Z","shell.execute_reply.started":"2024-10-09T13:59:26.776032Z"},"id":"PhdQs5bnrDNd","trusted":true},"outputs":[],"source":["def DilatedSpatialPyramidPooling(dspp_input):\n","    dims = dspp_input.shape\n","    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n","    x = convolution_block(x, kernel_size=1, use_bias=True)\n","    out_pool = layers.UpSampling2D(\n","        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",)(x)\n","\n","    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n","    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n","    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n","    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n","\n","    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n","    output = convolution_block(x, kernel_size=1)\n","    return output\n","\n","\n"]},{"cell_type":"code","execution_count":110,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-10-09T13:59:26.787060Z","iopub.status.busy":"2024-10-09T13:59:26.786747Z","iopub.status.idle":"2024-10-09T13:59:29.098213Z","shell.execute_reply":"2024-10-09T13:59:29.097464Z","shell.execute_reply.started":"2024-10-09T13:59:26.787030Z"},"id":"2e0ojqIBvuxU","outputId":"a82592b2-8c88-439b-c6bd-cd7b3bd3903e","trusted":true},"outputs":[{"ename":"ValueError","evalue":"A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[110], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(num_classes, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39mmodel_input, outputs\u001b[38;5;241m=\u001b[39mmodel_output)\n\u001b[0;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDeeplabV3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n","Cell \u001b[0;32mIn[110], line 6\u001b[0m, in \u001b[0;36mDeeplabV3\u001b[0;34m(image_size, num_classes)\u001b[0m\n\u001b[1;32m      3\u001b[0m resnet50 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mapplications\u001b[38;5;241m.\u001b[39mResNet50(\n\u001b[1;32m      4\u001b[0m     weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_tensor\u001b[38;5;241m=\u001b[39mmodel_input)\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m resnet50\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv4_block6_2_relu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moutput\n\u001b[0;32m----> 6\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mDilatedSpatialPyramidPooling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m input_a \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mUpSampling2D(\n\u001b[1;32m      9\u001b[0m     size\u001b[38;5;241m=\u001b[39m(image_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], image_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m     10\u001b[0m     interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,)(x)\n\u001b[1;32m     11\u001b[0m input_b \u001b[38;5;241m=\u001b[39m resnet50\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv2_block3_2_relu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moutput\n","Cell \u001b[0;32mIn[109], line 4\u001b[0m, in \u001b[0;36mDilatedSpatialPyramidPooling\u001b[0;34m(dspp_input)\u001b[0m\n\u001b[1;32m      2\u001b[0m dims \u001b[38;5;241m=\u001b[39m dspp_input\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mAveragePooling2D(pool_size\u001b[38;5;241m=\u001b[39m(dims[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], dims[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]))(dspp_input)\n\u001b[0;32m----> 4\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mconvolution_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m out_pool \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mUpSampling2D(\n\u001b[1;32m      6\u001b[0m     size\u001b[38;5;241m=\u001b[39m(dims[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dims[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,)(x)\n\u001b[1;32m      8\u001b[0m out_1 \u001b[38;5;241m=\u001b[39m convolution_block(dspp_input, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dilation_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","Cell \u001b[0;32mIn[108], line 5\u001b[0m, in \u001b[0;36mconvolution_block\u001b[0;34m(block_input, num_filters, kernel_size, dilation_rate, padding, use_bias)\u001b[0m\n\u001b[1;32m      2\u001b[0m     x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mConv2D(num_filters, kernel_size\u001b[38;5;241m=\u001b[39mkernel_size, dilation_rate \u001b[38;5;241m=\u001b[39m dilation_rate, padding\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_bias\u001b[38;5;241m=\u001b[39m use_bias,kernel_initializer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39mHeNormal(),\n\u001b[1;32m      3\u001b[0m )(block_input)\n\u001b[1;32m      4\u001b[0m     x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mBatchNormalization()(x)\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/python_code/hoc_deeplearning/MiAI_Defect_Detection/unet/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py:11594\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(features, name)\u001b[0m\n\u001b[1;32m  11592\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m  11593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m> 11594\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrelu_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11595\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11596\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m  11597\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n","File \u001b[0;32m~/python_code/hoc_deeplearning/MiAI_Defect_Detection/unet/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py:11635\u001b[0m, in \u001b[0;36mrelu_eager_fallback\u001b[0;34m(features, name, ctx)\u001b[0m\n\u001b[1;32m  11634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrelu_eager_fallback\u001b[39m(features: Annotated[Any, TV_Relu_T], name, ctx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Annotated[Any, TV_Relu_T]:\n\u001b[0;32m> 11635\u001b[0m   _attr_T, (features,) \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11636\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [features]\n\u001b[1;32m  11637\u001b[0m   _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T)\n","File \u001b[0;32m~/python_code/hoc_deeplearning/MiAI_Defect_Detection/unet/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:251\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[0;32m--> 251\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[1;32m    253\u001b[0m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[1;32m    254\u001b[0m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n","File \u001b[0;32m~/python_code/hoc_deeplearning/MiAI_Defect_Detection/unet/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:209\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    207\u001b[0m overload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__tf_tensor__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moverload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[1;32m    212\u001b[0m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[1;32m    213\u001b[0m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/python_code/hoc_deeplearning/MiAI_Defect_Detection/unet/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"]}],"source":["def DeeplabV3(image_size, num_classes):\n","    model_input = keras.Input(shape=(image_size, image_size, 3))\n","    resnet50 = keras.applications.ResNet50(\n","        weights=\"imagenet\", include_top=False, input_tensor=model_input)\n","    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n","    x = DilatedSpatialPyramidPooling(x)\n","\n","    input_a = layers.UpSampling2D(\n","        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n","        interpolation=\"bilinear\",)(x)\n","    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n","    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n","\n","    x = layers.Concatenate(axis=-1)([input_a, input_b])\n","    x = convolution_block(x)\n","    x = convolution_block(x)\n","    x = layers.UpSampling2D(\n","        size=(image_size // x.shape[1], image_size // x.shape[2]),\n","        interpolation=\"bilinear\",)(x)\n","    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n","    return keras.Model(inputs=model_input, outputs=model_output)\n","\n","\n","model = DeeplabV3(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## model compile"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T13:59:29.099517Z","iopub.status.busy":"2024-10-09T13:59:29.099236Z"},"id":"Mbwn-z9B1E0f","outputId":"149cfd06-6c5d-4f31-c2af-15e4cfe45df0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","250/250 [==============================] - 86s 216ms/step - loss: 1.1972 - accuracy: 0.6372 - val_loss: 1.6667 - val_accuracy: 0.5656\n","Epoch 2/100\n","250/250 [==============================] - 53s 213ms/step - loss: 0.9470 - accuracy: 0.7006 - val_loss: 1.1476 - val_accuracy: 0.6379\n","Epoch 3/100\n","250/250 [==============================] - 53s 213ms/step - loss: 0.8372 - accuracy: 0.7319 - val_loss: 1.0149 - val_accuracy: 0.6829\n","Epoch 4/100\n","250/250 [==============================] - 53s 213ms/step - loss: 0.7577 - accuracy: 0.7566 - val_loss: 1.0219 - val_accuracy: 0.6758\n","Epoch 5/100\n","175/250 [====================>.........] - ETA: 15s - loss: 0.7395 - accuracy: 0.7636"]}],"source":["loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = keras.optimizers.Adam(learning_rate=0.001)\n","model.compile(optimizer= optimizer , loss=loss, metrics=['accuracy'])\n","\n","history = model.fit(train_dataset,validation_data=val_dataset, epochs=10 )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2N223_Z2JXh","trusted":true},"outputs":[],"source":["from scipy.io import loadmat\n","import numpy as np\n","\n","colormap = loadmat('/kaggle/input/multiperson/instance-level_human_parsing/human_colormap.mat'\n",")['colormap']\n","\n","colormap = colormap * 100\n","colormap = colormap.astype(np.int8)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dx0qwyYG274L","trusted":true},"outputs":[],"source":["def infer(model, image_tensor):\n","    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n","    predictions = np.squeeze(predictions)\n","    predictions = np.argmax(predictions, axis=2)\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZrYIBW7283h","trusted":true},"outputs":[],"source":["def decode_segmentation_masks(mask, colormap, n_classes):\n","    r = np.zeros_like(mask).astype(np.uint8)\n","    g = np.zeros_like(mask).astype(np.uint8)\n","    b = np.zeros_like(mask).astype(np.uint8)\n","\n","\n","    for l in range(0, n_classes):\n","        idx = mask == l\n","        r[idx] = colormap[l, 0]\n","        g[idx] = colormap[l, 1]\n","        b[idx] = colormap[l, 2]\n","\n","    rgb = np.stack([r, g, b], axis = 2)\n","    return rgb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KCuxmtK6TMO","trusted":true},"outputs":[],"source":["def get_overlay(image, colored_mask):\n","  #image = tf.keras.utils.array_to_img(image)\n","    image = np.array(image).astype(np.uint8)\n","    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0) #  0.35 image + 0.65 cm + 0\n","    return overlay\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tjzej8Om7XwY","trusted":true},"outputs":[],"source":["def plot_samples_matplotlib(display_list, figsize=(5, 3)):\n","    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n","    for i in range(len(display_list)):\n","        if display_list[i].shape[-1] == 3 :\n","            axes[i].imshow(tf.keras.utils.array_to_img(display_list[i]))\n","        else:\n","            axes[i].imshow(display_list[i])\n","\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxHFBJwf8fHB","trusted":true},"outputs":[],"source":["def plot_predictions(image_list, colormap, model):\n","    for image_file in image_list:\n","        image_tensor = read_image(image_file)\n","        prediction_mask = infer(image_tensor= image_tensor, model= model)\n","        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 20)\n","        overlay = get_overlay(image_tensor, prediction_colormap)\n","        plot_samples_matplotlib([image_tensor, overlay, prediction_colormap], figsize=(18, 14))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdG4Zksl9Z9t","outputId":"d7825f2e-b4ea-4199-e2cc-de89ee734d92","trusted":true},"outputs":[],"source":["plot_predictions(train_images[:4], colormap, model=model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vf6KPsNh9q0Q"},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":735698,"sourceId":1276217,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
